{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 練習\n",
    "\n",
    "- 觀察 https://afuntw.github.io/Test-Crawling-Website/pages/blog/index.html 並撰寫爬蟲程式\n",
    "- request 附上 User-Agent 資訊\n",
    "- 下載網站上每個網頁的標題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from urllib.parse import urljoin\n",
    "from pprint import pprint\n",
    "\n",
    "url = 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/index.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fu = UserAgent()\n",
    "headers = {'User-Agent': fu.random}\n",
    "resp = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(resp.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_list = []\n",
    "view_list = []\n",
    "links = soup.find_all('a')\n",
    "links = [link['href'] for link in links]\n",
    "links = [urljoin(resp.url, link) for link in links]\n",
    "links = list(set(links))\n",
    "wait_list += links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://afuntw.github.io/Test-Crawling-Website/pages/blog/post.html\n",
      "wait list:\n",
      "['https://afuntw.github.io/Test-Crawling-Website/pages/blog/index.html',\n",
      " 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/contact.html',\n",
      " 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/about.html']\n",
      "view list:\n",
      "['https://afuntw.github.io/Test-Crawling-Website/pages/blog/post.html']\n",
      "all text:\n",
      "['Man must explore, and this is exploration at its greatest']\n",
      "=======================================================================================\n",
      "https://afuntw.github.io/Test-Crawling-Website/pages/blog/about.html\n",
      "wait list:\n",
      "['https://afuntw.github.io/Test-Crawling-Website/pages/blog/index.html',\n",
      " 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/contact.html']\n",
      "view list:\n",
      "['https://afuntw.github.io/Test-Crawling-Website/pages/blog/post.html',\n",
      " 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/about.html']\n",
      "all text:\n",
      "['Man must explore, and this is exploration at its greatest', 'About Me']\n",
      "=======================================================================================\n",
      "https://afuntw.github.io/Test-Crawling-Website/pages/blog/contact.html\n",
      "wait list:\n",
      "['https://afuntw.github.io/Test-Crawling-Website/pages/blog/index.html']\n",
      "view list:\n",
      "['https://afuntw.github.io/Test-Crawling-Website/pages/blog/post.html',\n",
      " 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/about.html',\n",
      " 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/contact.html']\n",
      "all text:\n",
      "['Man must explore, and this is exploration at its greatest',\n",
      " 'About Me',\n",
      " 'Contact Me']\n",
      "=======================================================================================\n",
      "https://afuntw.github.io/Test-Crawling-Website/pages/blog/index.html\n",
      "wait list:\n",
      "[]\n",
      "view list:\n",
      "['https://afuntw.github.io/Test-Crawling-Website/pages/blog/post.html',\n",
      " 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/about.html',\n",
      " 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/contact.html',\n",
      " 'https://afuntw.github.io/Test-Crawling-Website/pages/blog/index.html']\n",
      "all text:\n",
      "['Man must explore, and this is exploration at its greatest',\n",
      " 'About Me',\n",
      " 'Contact Me',\n",
      " 'Clean Blog']\n",
      "=======================================================================================\n"
     ]
    }
   ],
   "source": [
    "all_h1_text = []\n",
    "\n",
    "while wait_list:\n",
    "\n",
    "    link = wait_list.pop()\n",
    "    if link in view_list:\n",
    "        continue\n",
    "    \n",
    "    print(link)\n",
    "    view_list.append(link)\n",
    "    \n",
    "    page_resp = requests.get(link, headers=headers)\n",
    "    page_soup = BeautifulSoup(page_resp.text, 'lxml')\n",
    "    \n",
    "    # get h1 tag on current page\n",
    "    h1s = page_soup.find_all('h1')\n",
    "    h1s = [h1.text for h1 in h1s]\n",
    "    all_h1_text += h1s\n",
    "    \n",
    "    # search new links in current page\n",
    "    links = page_soup.find_all('a')\n",
    "    links = [link['href'] for link in links]\n",
    "    links = [urljoin(page_resp.url, link) for link in links]\n",
    "    links = list(filter(lambda x: x not in view_list, links))\n",
    "    wait_list += links\n",
    "    wait_list = list(set(wait_list))\n",
    "    print('wait list:')\n",
    "    pprint(wait_list)\n",
    "    print('view list:')\n",
    "    pprint(view_list)\n",
    "    print('all text:')\n",
    "    pprint(all_h1_text)\n",
    "    print('='*87)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
