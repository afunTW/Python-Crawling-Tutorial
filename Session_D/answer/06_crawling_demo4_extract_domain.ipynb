{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current website: https://afuntw.github.io/demo-crawling/demo-page/ex4/index1.html\n",
      "root_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "current_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "root_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "current_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "root_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "current_url extract: ExtractResult(subdomain='www', domain='facebook', suffix='com')\n",
      "Get h2 tags: ['Home Heading 1', 'Home Heading 2', 'Home Heading 3', \"First featurette heading. It'll blow your mind.\", \"Oh yeah, it's that good. See for yourself.\", 'And lastly, this one. Checkmate.']\n",
      "URL wait list: ['https://afuntw.github.io/demo-crawling/demo-page/ex4/index2.html', 'https://afuntw.github.io/demo-crawling/demo-page/ex4/index3.html']\n",
      "URL viewed list: ['https://afuntw.github.io/demo-crawling/demo-page/ex4/index1.html']\n",
      "\n",
      "Current website: https://afuntw.github.io/demo-crawling/demo-page/ex4/index2.html\n",
      "root_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "current_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "root_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "current_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "root_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "current_url extract: ExtractResult(subdomain='', domain='github', suffix='com')\n",
      "Get h2 tags: ['Home Heading 1', 'Home Heading 2', 'Home Heading 3', \"First featurette heading. It'll blow your mind.\", \"Oh yeah, it's that good. See for yourself.\", 'And lastly, this one. Checkmate.', 'About Heading 1', 'About Heading 2', 'About Heading 3']\n",
      "URL wait list: ['https://afuntw.github.io/demo-crawling/demo-page/ex4/index3.html']\n",
      "URL viewed list: ['https://afuntw.github.io/demo-crawling/demo-page/ex4/index1.html', 'https://afuntw.github.io/demo-crawling/demo-page/ex4/index2.html']\n",
      "\n",
      "Current website: https://afuntw.github.io/demo-crawling/demo-page/ex4/index3.html\n",
      "root_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "current_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "root_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "current_url extract: ExtractResult(subdomain='afuntw', domain='github', suffix='io')\n",
      "Get h2 tags: ['Home Heading 1', 'Home Heading 2', 'Home Heading 3', \"First featurette heading. It'll blow your mind.\", \"Oh yeah, it's that good. See for yourself.\", 'And lastly, this one. Checkmate.', 'About Heading 1', 'About Heading 2', 'About Heading 3', 'Contact Heading 1', 'Contact Heading 2', 'Contact Heading 3', 'Reminds. If you have any problems or suggestions, please contact me, thank you.']\n",
      "URL wait list: []\n",
      "URL viewed list: ['https://afuntw.github.io/demo-crawling/demo-page/ex4/index1.html', 'https://afuntw.github.io/demo-crawling/demo-page/ex4/index2.html', 'https://afuntw.github.io/demo-crawling/demo-page/ex4/index3.html']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from tldextract import extract\n",
    "\n",
    "wait_list = ['https://afuntw.github.io/demo-crawling/demo-page/ex4/index1.html']\n",
    "viewed_list = []\n",
    "h2_answer = []\n",
    "\n",
    "# 當 wait list 裏面還有網址發生的情況\n",
    "while wait_list != []:\n",
    "\n",
    "    # 取出 wait list 裏面的第一個網址\n",
    "    url = wait_list.pop(0)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    print('Current website: {}'.format(url))\n",
    "\n",
    "    # 將當前頁面存入已經看過的清單\n",
    "    viewed_list.append(url)\n",
    "\n",
    "    # 取得當前頁面中的 h2 tag 並將結果存入 h2_answer\n",
    "    h2 = soup.find_all('h2')\n",
    "    for tag in h2:\n",
    "        h2_answer.append(tag.text)\n",
    "\n",
    "    # 取得頁面中的 a tag\n",
    "    links = soup.find_all('a', href=True)\n",
    "    for link in links:\n",
    "\n",
    "        new_url = urljoin(url, link['href'])\n",
    "\n",
    "        # 過濾錨點, 不需要再對相同的網頁送 request\n",
    "        check_anchor = not re.match('#.*', link['href'])\n",
    "\n",
    "        # 過濾程式碼\n",
    "        check_code = not re.match('^javascript.*', link['href'])\n",
    "\n",
    "        # 過濾協定, 只取 http 或是 https\n",
    "        # Hint: 若原本 href 是相對路徑則沒有協定, 要先透過 urljoin 取得絕對路徑\n",
    "        check_protocol = urlparse(new_url).scheme in ['http', 'https']\n",
    "\n",
    "        # 實際過濾的判斷式\n",
    "        if check_anchor and check_code and check_protocol:\n",
    "\n",
    "            # 對當前 url 與新的 url 做 extract 分析網域\n",
    "            root_url = extract(url)\n",
    "            current_url = extract(new_url)\n",
    "            print('root_url extract: {}'.format(root_url))\n",
    "            print('current_url extract: {}'.format(current_url))\n",
    "\n",
    "            # 檢查 subdomain 是 www 或是與當前頁面的 subdomain 相同\n",
    "            check_subdomain = current_url.subdomain == 'www' or current_url.subdomain == root_url.subdomain\n",
    "\n",
    "            # 檢查新的 url 要與當前頁面的 domain 相同, 且符合 subdomain 需求\n",
    "            if root_url.domain == current_url.domain and check_subdomain:\n",
    "\n",
    "                # 新的 url 要符合的條件\n",
    "                # 1. wait_list 裏面沒有出現\n",
    "                # 2. viewed_list 也沒有出現\n",
    "                if new_url not in wait_list and new_url not in viewed_list:\n",
    "\n",
    "                    # 將新發現的超連結存入 wait list\n",
    "                    wait_list.append(new_url)\n",
    "\n",
    "    print('Get h2 tags: {}'.format(h2_answer))\n",
    "    print('URL wait list: {}'.format(wait_list))\n",
    "    print('URL viewed list: {}'.format(viewed_list))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
